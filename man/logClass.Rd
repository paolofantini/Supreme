% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/logClass.R
\name{logClass}
\alias{logClass}
\title{Internal Supreme function}
\usage{
logClass(predictors, target, inTraining, train.glmnet = FALSE)
}
\arguments{
\item{predictors}{the matrix of predictors, i. e. the posterior topic proportions for each doc.}

\item{target}{factor, the classification variable.}

\item{inTraining}{the numeric ids of documents belonging to the training set.}

\item{train.glmnet}{logical. If \code{TRUE} runs \code{train.glmnet} function from package \pkg{caret}. Default is \code{FALSE}.}
}
\value{
\code{err} list of misclassification errors (error = 1 - Accuracy) and confusion matrices (from package \pkg{caret}):
\item{e0.train}{train error from method "glmnet"}
\item{e1.train}{train error from method "cv.glmnet"}
\item{e2.train}{train error from method "train.glmnet"}
\item{e0.test}{test error from method "predict.glmnet"}
\item{e1.test}{test error from method "predict.cv.glmnet"}
\item{e2.test}{test error from method "predict.train.glmnet"}
\item{cm0}{confusion matrix for method "glmnet"}
\item{cm1}{confusion matrix for method "cv.glmnet"}
\item{cm1}{confusion matrix for method "train.glmnet"}
}
\description{
\code{logClass} fits logistic classification models from \pkg{glmnet} and \pkg{caret} packages to the
\code{topic.posteriors} matrix of predictors trained by LDA topic model.
A \code{target} variable is used as a supervision variable.
\code{logClass} is called by \code{\link{mcLDA}} function.
}
\details{
This function applies three different methods.
\emph{Method0} and \emph{Method1} respectively refer to functions \code{glmnet} and \code{cv.glmnet} from package \pkg{glmnet}.
\emph{Method2} refers to function \code{train.glmnet} from package \pkg{caret}.
\emph{Method0} tends to overfit the training set.
\emph{Method1} and \emph{Method2} try to avoid overfitting problems using cross-validation.
\emph{Method2} uses repeated cross-validation and is more stable than \emph{Method1} but much more time-consuming.
}
\note{
Tuning parameters are \code{alpha} and \code{lambda}.
\emph{Method0} and \emph{Method1} pick no value for \code{alpha} and it remains at default value \code{alpha = 1}.
\emph{Method2} selects values for \code{alpha} and \code{lambda} using the tuning parameter grid defined by
\code{expand.grid(alpha = seq(0.1, 1, 0.1), lambda = glmnetFit0$lambda)}.
More details can be found \href{http://caret.r-forge.r-project.org/training.html}{here}.
In \emph{Method1} the best model is selected using the "one standard error rule":
default best value of the penalty parameter \code{lambda} is \code{s = "lambda.1se"}, stored on the \code{cv.glmnet} object.
Such a rule takes a conservative approach. Alternatively \code{s = "lambda.min"} can be used.
Full details are given in \emph{"The Elements of Statistical Learnings"} (T. Hastie, R. Tibshirani, J. Friedman) 2nd edition p. 61.
}
\examples{
\dontrun{
library(Supreme)
library(topicmodels)
library(caret)

# Input data.
data("dtm")
data("classes")

# Reduced dtm.lognet
dtm.lognet <- reduce_dtm(dtm, method = "lognet", target = classes, export = TRUE)

# Run a 35-topic model over the reduced dtm.lognet and compute topic posteriors.
ldaVEM.mod <- LDA(dtm.lognet$reduced, k = 35, method = "VEM", control = list(seed = 2014))
topic.posteriors <- as.data.frame(posterior(ldaVEM.mod)$topics)

# Misclassification error.
set.seed(2010)  # for inTraining reproducibility
inTraining <- createDataPartition(classes, p = 0.75, list = FALSE)  # for balancing the size of target classes in training set
mis.error <- logClass(topic.posteriors, classes, inTraining)
}
}

